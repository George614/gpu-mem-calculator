Metadata-Version: 2.4
Name: gpu-mem-calculator
Version: 0.1.0
Summary: GPU Memory Calculator for LLM Training
Author: GPU Mem Calculator Team
License: MIT
Project-URL: Homepage, https://github.com/George614/gpu-mem-calculator
Project-URL: Repository, https://github.com/George614/gpu-mem-calculator
Project-URL: Issues, https://github.com/George614/gpu-mem-calculator/issues
Keywords: gpu,memory,calculator,llm,large-language-model,training,deepspeed,megatron,pytorch,fsdp,transformer,machine-learning,deep-learning,distributed-training,zero-optimization
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic>=2.0.0
Requires-Dist: click>=8.1.0
Requires-Dist: pydantic-settings>=2.0.0
Requires-Dist: rich>=13.0.0
Provides-Extra: web
Requires-Dist: fastapi>=0.100.0; extra == "web"
Requires-Dist: uvicorn[standard]>=0.23.0; extra == "web"
Requires-Dist: jinja2>=3.1.0; extra == "web"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"
Dynamic: license-file

# GPU Memory Calculator for LLM Training

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

A versatile Python application for calculating GPU memory requirements for training Large Language Models with support for multiple training engines including PyTorch DDP, DeepSpeed ZeRO, Megatron-LM, and FSDP.

üìñ **[Getting Started Guide](docs/GETTING_STARTED.md)** | üí¨ **[FAQ](docs/FAQ.md)** | ü§ù **[Contributing](CONTRIBUTING.md)**

<p align="center">
  <img src="screenshot.png" alt="GPU Memory Calculator Screenshot" width="800">
</p>

## üöÄ Why Use This Tool?

Training large language models requires careful memory planning. This calculator helps you:

- **üí∞ Save costs** by determining the optimal GPU configuration before you start training
- **‚ö° Avoid OOM errors** by validating your training configuration fits in GPU memory
- **üìä Compare strategies** across different training engines (DeepSpeed, Megatron, FSDP)
- **üéØ Plan infrastructure** by knowing exactly how many GPUs you need
- **üìà Scale efficiently** with detailed memory breakdowns for optimization

Whether you're training a 7B parameter model on a single GPU or a 175B model across hundreds of GPUs, this tool provides accurate memory estimates based on proven formulas from DeepSpeed, Megatron-LM, and the latest research.

## ‚ú® Features

- üîß **Multiple Training Engines**: Support for PyTorch DDP, DeepSpeed ZeRO (stages 1-3), Megatron-LM, Megatron+DeepSpeed, and PyTorch FSDP
- üñ•Ô∏è **Dual Interface**: Both CLI and Web UI for flexible usage
- üéØ **Preset Models**: Quick-load configurations for popular models (LLaMA 2, GPT-3, etc.)
- üìä **Detailed Breakdown**: Memory breakdown by component (parameters, gradients, optimizer states, activations)
- ‚úÖ **Feasibility Analysis**: Check if your configuration fits on available GPU memory
- ‚öôÔ∏è **Easy Config**: JSON-based configuration files with human-readable parameter formats

## üì¶ Installation

### Quick Start

The fastest way to get started:

```bash
pip install git+https://github.com/George614/gpu-mem-calculator.git
```

### From source

```bash
git clone https://github.com/George614/gpu-mem_calculator.git
cd gpu_mem_calculator
pip install -e .
```

### For Web UI support

```bash
pip install -e ".[web]"
```

### Development installation

```bash
pip install -e ".[dev]"
```

## üéì Use Cases

### Research & Academia
- Estimate GPU requirements for research projects before requesting compute resources
- Plan multi-GPU training configurations for large-scale experiments
- Compare memory efficiency of different training strategies

### Industry & Production
- Cost optimization: Choose the right GPU type and count for your training workload
- Capacity planning: Forecast infrastructure needs for model development
- Debugging: Diagnose OOM errors and optimize memory usage

### Education & Learning
- Understand how training configuration affects memory consumption
- Learn about different distributed training strategies
- Experiment with various optimization techniques safely

## üöÄ Usage

### Command Line Interface

#### Using model presets (Recommended)

The calculator includes pre-configured model presets for popular LLMs:

```bash
# List all available presets
gpu-mem-calc presets

# Calculate with a preset
gpu-mem-calc calculate --preset llama2-7b
gpu-mem-calc calculate --preset mixtral-8x7b --format json

# List presets in table format
gpu-mem-calc presets --format table
```

Available presets include:
- **Dense Models**: LLaMA 2 (7B, 13B, 70B), GPT-3 (175B)
- **MoE Models**: Mixtral 8x7B, GLM-4 (9B), GLM-4.7 (355B), GLM-4.5 Air (106B),
  Qwen1.5-MoE-A2.7B, DeepSeek-MoE (16B)

#### Calculate from config file

```bash
gpu-mem-calc calculate --config configs/llama2_7b_deepspeed.json
```

#### Quick calculation from model size

```bash
# Calculate memory for 7B model with 8x80GB GPUs using DeepSpeed
gpu-mem-calc quick 7 --gpus 8 --engine deepspeed

# With custom GPU memory
gpu-mem-calc quick 70 --gpus 64 --gpu-mem 80 --engine megatron
```

#### Validate configuration

```bash
gpu-mem-calc validate configs/my_config.json
```

### Web Interface

Start the web server:

```bash
python -m gpu_mem_calculator.web.app
```

Or using uvicorn directly:

```bash
uvicorn gpu_mem_calculator.web.app:app --reload
```

Then open your browser to `http://localhost:8000`

### Python API

```python
from gpu_mem_calculator.core.calculator import GPUMemoryCalculator
from gpu_mem_calculator.core.models import (
    ModelConfig,
    TrainingConfig,
    ParallelismConfig,
    EngineConfig,
    GPUConfig,
)

# Create configuration
model_config = ModelConfig(
    name="llama2-7b",
    num_parameters=7_000_000_000,
    num_layers=32,
    hidden_size=4096,
    num_attention_heads=32,
    vocab_size=32000,
    max_seq_len=4096,
)

training_config = TrainingConfig(
    batch_size=4,
    gradient_accumulation_steps=4,
    dtype="bf16",
    optimizer="adamw",
)

parallelism_config = ParallelismConfig(
    data_parallel_size=8,
)

engine_config = EngineConfig(
    type="deepspeed",
    zero_stage=3,
    offload_optimizer="cpu",
)

gpu_config = GPUConfig(
    num_gpus=8,
    gpu_memory_gb=80,
)

# Calculate memory
calculator = GPUMemoryCalculator(
    model_config=model_config,
    training_config=training_config,
    parallelism_config=parallelism_config,
    engine_config=engine_config,
    gpu_config=gpu_config,
)

result = calculator.calculate()

print(f"Memory per GPU: {result.total_memory_per_gpu_gb:.2f} GB")
print(f"Fits on GPU: {result.fits_on_gpu}")
print(f"Utilization: {result.memory_utilization_percent:.1f}%")
```

## Configuration File Format

```json
{
  "model": {
    "name": "llama2-7b",
    "num_parameters": "7B",
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 32000,
    "max_seq_len": 4096
  },
  "training": {
    "batch_size": 4,
    "gradient_accumulation_steps": 4,
    "optimizer": "adamw",
    "dtype": "bf16",
    "activation_checkpointing": 1
  },
  "parallelism": {
    "tensor_parallel_size": 1,
    "pipeline_parallel_size": 1,
    "data_parallel_size": 8,
    "sequence_parallel": false
  },
  "engine": {
    "type": "deepspeed",
    "zero_stage": 3,
    "offload_optimizer": "cpu",
    "offload_param": "none"
  },
  "hardware": {
    "num_gpus": 8,
    "gpu_memory_gb": 80
  }
}
```

## Supported Training Engines

### PyTorch DDP (Baseline)
Standard Distributed Data Parallel training without memory optimizations.

### DeepSpeed ZeRO
- **ZeRO-1**: Shard optimizer states
- **ZeRO-2**: Shard optimizer states + gradients
- **ZeRO-3**: Shard everything (parameters, gradients, optimizer states)
- Supports CPU/NVMe offloading

### Megatron-LM
Tensor and pipeline parallelism with activation checkpointing support.

### Megatron + DeepSpeed
Combines Megatron-LM's model parallelism with DeepSpeed ZeRO's optimizer sharding.

### PyTorch FSDP
Fully Sharded Data Parallel with multiple sharding strategies.

## Memory Formulas

The calculator uses the following formulas based on training engine:

**Base Components:**
- Model Parameters: `num_params √ó bytes_per_param` (FP16/BF16 = 2 bytes, FP32 = 4 bytes)
- Gradients: `num_params √ó bytes_per_param` (same precision as parameters during training)
- Optimizer States: `num_params √ó 12` for Adam/AdamW (FP32 param copy + momentum + variance)
- Activations: `batch_size √ó seq_len √ó hidden_size √ó num_layers √ó 16` (heuristic approximation)

**DeepSpeed ZeRO Stages:**

*ZeRO-1* (shard optimizer states):
```
total_per_gpu = 4 √ó params + (12 √ó params) / num_gpus
```

*ZeRO-2* (shard optimizer + gradients):
```
total_per_gpu = 2 √ó params + (2 √ó params) / num_gpus + (12 √ó params) / num_gpus
```

*ZeRO-3* (shard everything):
```
total_per_gpu = largest_layer_memory + (16 √ó params) / num_gpus
where largest_layer_memory = 4 √ó largest_layer_params
```

Note: ZeRO-3 uses 16 bytes (not 18) when offloading is disabled, representing:
- 12 bytes: Optimizer states (FP32)
- 2 bytes: Sharded FP16 parameters
- 2 bytes: Sharded FP16 gradients

**References:**
- [DeepSpeed Memory Documentation](https://deepspeed.readthedocs.io/en/latest/memory.html)
- [Microsoft Research ZeRO Blog](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
- [EleutherAI Transformer Math 101](https://blog.eleuther.ai/transformer-math/)

## Example Configurations

### LLaMA 2 7B with DeepSpeed ZeRO-3
```bash
gpu-mem-calc calculate --config configs/llama2_7b_deepspeed.json
```

### GPT-3 175B with Megatron-LM
```bash
gpu-mem-calc calculate --config configs/gpt3_175b_megatron.json
```

### Custom 1B model with PyTorch DDP
```bash
gpu-mem-calc calculate --config configs/pytorch_ddp_example.json
```

## Web UI Features

- **Interactive Form**: Easy-to-use interface for tweaking hyperparameters
- **Unified Presets**: Same model presets available in CLI and web interface
- **Real-time Validation**: Instant feedback on configuration validity
- **Visual Breakdown**: Bar chart showing memory component distribution
- **Feasibility Indicators**: Color-coded memory utilization status
- **Export Options**: Save config as JSON or copy to clipboard

## Development

### Running Tests

```bash
pytest tests/
```

### Code Formatting

```bash
black src/ cli/ web/
ruff check src/ cli/ web/
```

### Type Checking

```bash
mypy src/
```

## üìù Contributing

Contributions are welcome! Please feel free to submit a Pull Request. See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## üìö References

The memory calculations in this tool are based on authoritative sources:

**Core Memory Formulas:**
- [EleutherAI Transformer Math 101](https://blog.eleuther.ai/transformer-math/) - Comprehensive breakdown of transformer memory requirements
- [Microsoft Research ZeRO Blog](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) - ZeRO optimization techniques
- [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2204.13323) - Activation checkpointing strategies

**Engine Documentation:**
- [DeepSpeed Memory Documentation](https://deepspeed.readthedocs.io/en/latest/memory.html) - Official DeepSpeed memory formulas
- [NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - Tensor and pipeline parallelism
- [PyTorch FSDP Documentation](https://pytorch.org/docs/stable/fsdp.html) - Fully sharded data parallel
- [PyTorch DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) - Distributed data parallel

**Related Tools:**
- [llm-analysis](https://github.com/cli99/llm-analysis) - LLM memory analysis
- [vram-calculator](https://github.com/furiousteabag/vram-calculator) - VRAM calculation utilities

## ü§ù Community & Support

- üìñ [Documentation](README.md)
- üêõ [Issue Tracker](https://github.com/George614/gpu-mem-calculator/issues)
- üí¨ [Discussions](https://github.com/George614/gpu-mem-calculator/discussions)
- üìß Contact the maintainers via GitHub

### Star History

If you find this tool useful, please consider giving it a star! ‚≠ê

## üìã Roadmap

Planned features and improvements:

- [ ] PyPI package distribution
- [ ] Support for more model architectures (Vision Transformers, Diffusion models)
- [ ] Inference memory calculation
- [ ] Multi-node training configurations
- [ ] Integration with popular training frameworks
- [ ] Real-time memory monitoring dashboard
- [ ] Export to training configuration files

## üôè Acknowledgments

This tool was inspired by and builds upon the excellent work of:
- [DeepSpeed Memory Estimator](https://deepspeed.readthedocs.io/en/latest/memory.html) - ZeRO memory optimization formulas
- [llm-analysis](https://github.com/cli99/llm-analysis) - LLM memory analysis methodology
- [vram-calculator](https://github.com/furiousteabag/vram-calculator) - VRAM calculation approach

Special thanks to the EleutherAI community for their comprehensive [Transformer Math 101](https://blog.eleuther.ai/transformer-math/) guide, which provides detailed formulas for transformer memory calculations.

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.

## üìö Citation

If you use this tool in your research, please cite:

```bibtex
@software{gpu_mem_calculator,
  title = {GPU Memory Calculator for LLM Training},
  author = {GPU Mem Calculator Team},
  year = {2024},
  url = {https://github.com/George614/gpu-mem-calculator}
}
```

---

<p align="center">
  Made with ‚ù§Ô∏è for the ML community
</p>

<p align="center">
  <a href="https://github.com/George614/gpu-mem-calculator/stargazers">‚≠ê Star us on GitHub</a> ‚Ä¢
  <a href="https://github.com/George614/gpu-mem-calculator/issues">üêõ Report a Bug</a> ‚Ä¢
  <a href="https://github.com/George614/gpu-mem-calculator/issues">üí° Request a Feature</a>
</p>

